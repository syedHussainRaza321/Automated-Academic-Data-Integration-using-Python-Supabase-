{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IObQnQaSacwP",
        "outputId": "9c3af582-3773-44ce-901c-a67653e33d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.22.0)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.12/dist-packages (2.10.1)\n",
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.12/dist-packages (1.7.11)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: realtime in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-functions in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: storage3 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-auth in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: postgrest in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.13.5)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.4.3)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.2.18)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.12/dist-packages (from scholarly) (2.2.0)\n",
            "Requirement already satisfied: free-proxy in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.1.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.1.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.36.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.12/dist-packages (from scholarly) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.15.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow->scholarly) (2.9.0.post0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow->scholarly) (2.9.0.20251008)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->scholarly) (2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from bibtexparser->scholarly) (3.2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->scholarly) (1.17.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from free-proxy->scholarly) (5.4.0)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.11.10)\n",
            "Requirement already satisfied: yarl>=1.20.1 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (1.22.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime->supabase) (15.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.31.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (1.9.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions->supabase) (0.4.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.1.0->postgrest->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket<1.0,>=0.12.2->selenium->scholarly) (1.2.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (0.3.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install requests pdfplumber supabase pyjwt scholarly serpapi fuzzywuzzy[speedup] python-Levenshtein\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set these in Colab via `Edit > Notebook settings` or in a secure cell (only in your notebook)\n",
        "os.environ[\"SEMANTIC_SCHOLAR_KEY\"] = \"m9cTLqcFcI2TEbcB1K8i220RV6l4Jfvp366fxg5a\"\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://bjquctxzjruejzbdxjfr.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJqcXVjdHh6anJ1ZWp6YmR4amZyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjAwOTEyMjIsImV4cCI6MjA3NTY2NzIyMn0.MtsWvZDcHXlP-GkJc5OVEk2LKzxByyOHkE1CefiulOY\"\n",
        "os.environ[\"SERPAPI_KEY\"] = \"97cfe967c5d0cc8b81315b9fa6f55dbc02de7b832051d8e1b5b44a9fb111d827\"  # for SerpApi\n"
      ],
      "metadata": {
        "id": "Ixs-ap7mdYdY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests, time, json, urllib.parse\n",
        "\n",
        "SEM_KEY = os.environ.get(\"SEMANTIC_SCHOLAR_KEY\")\n",
        "BASE = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "def semantic_search(query, limit=20):\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"offset\": 0,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"paperId,title,authors,year,abstract,url,externalIds,openAccessPdf,citationCount\"\n",
        "    }\n",
        "    headers = {\"x-api-key\": SEM_KEY}  # docs vary; instructor said Bearer header â€” check your key type\n",
        "    # Some docs use Authorization: Bearer <key>. If that fails, try x-api-key per your key type.\n",
        "    r = requests.get(BASE, params=params, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    return data.get(\"data\", [])\n",
        "\n",
        "# Quick test\n",
        "papers = semantic_search(\"natural language processing\", limit=5)\n",
        "for p in papers:\n",
        "    print(p.get(\"title\"))\n",
        "    print(\"OA PDF:\", p.get(\"openAccessPdf\"))\n",
        "    print(\"DOI:\", p.get(\"externalIds\",{}).get(\"DOI\"))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4i45tymdYf6",
        "outputId": "be7d11cd-021f-43a5-f55e-f8238039cfcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\n",
            "OA PDF: {'url': 'https://dl.acm.org/doi/pdf/10.1145/3560815', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2107.13586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}\n",
            "DOI: 10.1145/3560815\n",
            "\n",
            "The Stanford CoreNLP Natural Language Processing Toolkit\n",
            "OA PDF: {'url': 'https://aclanthology.org/P14-5010.pdf', 'status': 'HYBRID', 'license': 'CCBY', 'disclaimer': 'Notice: Paper or abstract available at https://aclanthology.org/P14-5010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}\n",
            "DOI: 10.3115/v1/P14-5010\n",
            "\n",
            "Natural Language Processing (Almost) from Scratch\n",
            "OA PDF: {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/1103.0398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}\n",
            "DOI: 10.5555/1953048.2078186\n",
            "\n",
            "HuggingFace's Transformers: State-of-the-art Natural Language Processing\n",
            "OA PDF: {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/1910.03771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}\n",
            "DOI: None\n",
            "\n",
            "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\n",
            "OA PDF: {'url': 'https://arxiv.org/pdf/2007.15779', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: Paper or abstract available at https://arxiv.org/abs/2007.15779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}\n",
            "DOI: 10.1145/3458754\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, os\n",
        "SERPAPI_KEY = os.environ.get(\"SERPAPI_KEY\")\n",
        "SERPAPI_ENDPOINT = \"https://serpapi.com/search\"\n",
        "\n",
        "def serpapi_scholar_search(query, num=10):\n",
        "    params = {\n",
        "        \"engine\": \"google_scholar\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": num\n",
        "    }\n",
        "    r = requests.get(SERPAPI_ENDPOINT, params=params, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "res = serpapi_scholar_search(\"transformer language models\", num=10)\n",
        "# res will contain organic results with metadata (title, snippet, link)\n"
      ],
      "metadata": {
        "id": "hZOA8JxvdYiV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def normalize_title(t):\n",
        "    return ''.join(ch for ch in t.lower() if ch.isalnum() or ch.isspace()).strip()\n",
        "\n",
        "def merge_records(list_of_records):\n",
        "    merged = {}\n",
        "    for r in list_of_records:\n",
        "        doi = None\n",
        "        ext = r.get(\"externalIds\") or {}\n",
        "        doi = ext.get(\"DOI\") or r.get(\"doi\")  # handle different fields\n",
        "        title_key = doi if doi else normalize_title(r.get(\"title\",\"\"))[:200]\n",
        "        if title_key in merged:\n",
        "            # merge logic: append sources, choose non-empty fields\n",
        "            merged[title_key][\"sources\"].append(r.get(\"source\"))\n",
        "            # prefer longer abstract\n",
        "            if len(r.get(\"abstract\",\"\") or \"\") > len(merged[title_key].get(\"abstract\",\"\") or \"\"):\n",
        "                merged[title_key][\"abstract\"] = r.get(\"abstract\")\n",
        "            # keep pdf candidates list\n",
        "            merged[title_key][\"pdf_candidates\"].extend(r.get(\"pdf_candidates\",[]))\n",
        "        else:\n",
        "            merged[title_key] = {\n",
        "                \"title\": r.get(\"title\"),\n",
        "                \"authors\": [a.get(\"name\") for a in r.get(\"authors\",[])],\n",
        "                \"year\": r.get(\"year\"),\n",
        "                \"doi\": doi,\n",
        "                \"externalIds\": ext,\n",
        "                \"sources\": [r.get(\"source\")],\n",
        "                \"abstract\": r.get(\"abstract\"),\n",
        "                \"pdf_candidates\": r.get(\"pdf_candidates\",[])\n",
        "            }\n",
        "    return list(merged.values())\n"
      ],
      "metadata": {
        "id": "YEnhFzT8dYk9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from supabase import create_client\n",
        "import base64, os\n",
        "\n",
        "SUPABASE_URL = \"https://bjquctxzjruejzbdxjfr.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJqcXVjdHh6anJ1ZWp6YmR4amZyIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjAwOTEyMjIsImV4cCI6MjA3NTY2NzIyMn0.MtsWvZDcHXlP-GkJc5OVEk2LKzxByyOHkE1CefiulOY\"\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "def upload_pdf_to_supabase(bucket, path, pdf_bytes, content_type=\"application/pdf\"):\n",
        "    # path e.g., \"academic_pdfs/<paperid>.pdf\"\n",
        "    res = supabase.storage.from_(bucket).upload(path, pdf_bytes, {\"content-type\": content_type})\n",
        "    # res may include error structure; check\n",
        "    return res\n",
        "\n",
        "def insert_paper_row(table, metadata):\n",
        "    res = supabase.table(table).insert(metadata).execute()\n",
        "    return res"
      ],
      "metadata": {
        "id": "zPzKk_L9dYqN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber, io, requests\n",
        "\n",
        "def download_pdf_bytes(url):\n",
        "    r = requests.get(url, timeout=60, stream=True)\n",
        "    r.raise_for_status()\n",
        "    return r.content\n",
        "\n",
        "def extract_text_from_pdf_bytes(pdf_bytes, max_pages=None):\n",
        "    text = []\n",
        "    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
        "        pages = pdf.pages if max_pages is None else pdf.pages[:max_pages]\n",
        "        for p in pages:\n",
        "            try:\n",
        "                page_text = p.extract_text() or \"\"\n",
        "                text.append(page_text)\n",
        "            except Exception as e:\n",
        "                print(\"pdfplumber page error:\", e)\n",
        "    return \"\\n\".join(text)"
      ],
      "metadata": {
        "id": "Yry5NAW0dYnY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_records = [\n",
        "    {\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"authors\": [\"Ashish Vaswani\", \"Noam Shazeer\", \"Niki Parmar\", \"Jakob Uszkoreit\"],\n",
        "        \"year\": 2017,\n",
        "        \"doi\": \"10.48550/arXiv.1706.03762\",\n",
        "        \"externalIds\": {\"DOI\": \"10.48550/arXiv.1706.03762\"},\n",
        "        \"sources\": [\"SemanticScholar\"],\n",
        "        \"abstract\": \"The Transformer model relies entirely on attention mechanisms to draw global dependencies...\",\n",
        "        \"pdf_candidates\": [\"https://arxiv.org/pdf/1706.03762.pdf\"]\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "GOU0uw0WiZX5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from supabase import create_client\n",
        "import os\n",
        "\n",
        "SUPABASE_URL = \"https://bjquctxzjruejzbdxjfr.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJqcXVjdHh6anJ1ZWp6YmR4amZyIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MDA5MTIyMiwiZXhwIjoyMDc1NjY3MjIyfQ.llYmlbVzezEpcAMA5agzPkovdzFlVv0Kxy8s_ikXvmI\"  # use your actual key\n",
        "\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n"
      ],
      "metadata": {
        "id": "0fsK_wNAi0Wa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = \"academic_pdfs\"\n",
        "\n",
        "for rec in merged_records:\n",
        "    for pdf_url in rec[\"pdf_candidates\"]:\n",
        "        try:\n",
        "            print(\"ðŸ“„ Downloading PDF:\", pdf_url)\n",
        "            pdf_bytes = download_pdf_bytes(pdf_url)\n",
        "\n",
        "            # create filename\n",
        "            fname = (rec.get(\"doi\") or rec[\"title\"][:80]).replace(\"/\", \"_\").replace(\" \", \"_\") + \".pdf\"\n",
        "            path = f\"{fname}\"\n",
        "\n",
        "            print(\"â¬†ï¸ Uploading to Supabase...\")\n",
        "            upload_res = supabase.storage.from_(bucket).upload(path, pdf_bytes, {\"content-type\": \"application/pdf\"})\n",
        "            print(\"Upload response:\", upload_res)\n",
        "\n",
        "            print(\"ðŸ“– Extracting text...\")\n",
        "            text = extract_text_from_pdf_bytes(pdf_bytes, max_pages=2)  # only first 2 pages for test\n",
        "\n",
        "            excerpt = text[:1000] if text else None\n",
        "            metadata = {\n",
        "                \"title\": rec[\"title\"],\n",
        "                \"authors\": rec.get(\"authors\"),\n",
        "                \"year\": rec.get(\"year\"),\n",
        "                \"doi\": rec.get(\"doi\"),\n",
        "                \"source\": ','.join(rec.get(\"sources\",[])),\n",
        "                \"external_ids\": rec.get(\"externalIds\"),\n",
        "                \"supabase_path\": path,\n",
        "                \"pdf_downloaded\": True,\n",
        "                \"text_excerpt\": excerpt\n",
        "            }\n",
        "\n",
        "            print(\"ðŸ—‚ï¸ Inserting record into Supabase table...\")\n",
        "            insert_res = supabase.table(\"papers\").insert(metadata).execute()\n",
        "            print(\"âœ… Inserted:\", insert_res)\n",
        "            break  # stop after first successful PDF\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Error processing:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvjLAHvli0Rz",
        "outputId": "6e27917f-c9f2-4afd-c5c8-24cdd817d02f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Downloading PDF: https://arxiv.org/pdf/1706.03762.pdf\n",
            "â¬†ï¸ Uploading to Supabase...\n",
            "Upload response: UploadResponse(path=('10.48550_arXiv.1706.03762.pdf',), full_path='academic_pdfs/10.48550_arXiv.1706.03762.pdf', fullPath='academic_pdfs/10.48550_arXiv.1706.03762.pdf')\n",
            "ðŸ“– Extracting text...\n",
            "ðŸ—‚ï¸ Inserting record into Supabase table...\n",
            "âœ… Inserted: data=[{'id': '2f13c54e-0f77-4c30-ae69-4950cfb9a17d', 'title': 'Attention Is All You Need', 'authors': ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit'], 'year': 2017, 'doi': '10.48550/arXiv.1706.03762', 'source': 'SemanticScholar', 'external_ids': {'DOI': '10.48550/arXiv.1706.03762'}, 'supabase_path': '10.48550_arXiv.1706.03762.pdf', 'pdf_downloaded': True, 'text_excerpt': 'Providedproperattributionisprovided,Googleherebygrantspermissionto\\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\\nscholarlyworks.\\nAttention Is All You Need\\nAshishVaswaniâˆ— NoamShazeerâˆ— NikiParmarâˆ— JakobUszkoreitâˆ—\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJonesâˆ— AidanN.Gomezâˆ— â€  ÅukaszKaiserâˆ—\\nGoogleResearch UniversityofToronto GoogleBrain\\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\\nIlliaPolosukhinâˆ— â€¡\\nillia.polosukhin@gmail.com\\nAbstract\\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbesupe', 'created_at': '2025-10-12T08:44:02.531179+00:00'}] count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hereâ€™s a full code:\n",
        "fetches results from Semantic Scholar (primary) and SerpApi (Google Scholar) optionally,\n",
        "\n",
        "auto-creates a merged_records list by merging & deduplicating results (DOI-first, fuzzy-title fallback),\n",
        "\n",
        "downloads PDFs (if OA links present), extracts text with pdfplumber,\n",
        "\n",
        "ensures the Supabase storage bucket exists (creates if possible), uploads PDFs, and inserts metadata rows into a papers table,\n",
        "\n",
        "logs successes/failures and handles common errors (bucket missing, RLS unauthorized).\n"
      ],
      "metadata": {
        "id": "QlxWwMT8mfR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Install required packages (run once) ==========\n",
        "!pip install requests pdfplumber supabase scholarly serpapi fuzzywuzzy[speedup] python-Levenshtein\n",
        "\n",
        "# ========== Imports ==========\n",
        "import os, io, time, json, requests, re\n",
        "from pprint import pprint\n",
        "from fuzzywuzzy import fuzz\n",
        "import pdfplumber\n",
        "from supabase import create_client\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# ========== Configuration: set these in Colab before running ==========\n",
        "# Example (DO NOT hardcode in shared notebooks; use Colab secrets):\n",
        "# os.environ[\"SEMANTIC_SCHOLAR_KEY\"] = \"m9cTLqcFcI2TEbcB1K8i220RV6l4Jfvp366fxg5a\"\n",
        "# os.environ[\"SERPAPI_KEY\"] = \"97cfe967c5d0cc8b81315b9fa6f55dbc02de7b832051d8e1b5b44a9fb111d827\"\n",
        "# os.environ[\"SUPABASE_URL\"] = \"https://bjquctxzjruejzbdxjfr.supabase.co\"\n",
        "# os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJqcXVjdHh6anJ1ZWp6YmR4amZyIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MDA5MTIyMiwiZXhwIjoyMDc1NjY3MjIyfQ.llYmlbVzezEpcAMA5agzPkovdzFlVv0Kxy8s_ikXvmI\"\n",
        "SEM_KEY = os.environ.get(\"SEMANTIC_SCHOLAR_KEY\", \"\").strip()\n",
        "SERPAPI_KEY = os.environ.get(\"SERPAPI_KEY\", \"\").strip()\n",
        "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\", \"\").strip()\n",
        "SUPABASE_KEY = os.environ.get(\"SUPABASE_KEY\", \"\").strip()\n",
        "\n",
        "if not SEM_KEY:\n",
        "    print(\"âš ï¸ Warning: SEMANTIC_SCHOLAR_KEY not set. Semantic Scholar calls will fail.\")\n",
        "if not SUPABASE_URL or not SUPABASE_KEY:\n",
        "    raise RuntimeError(\"SUPABASE_URL and SUPABASE_KEY environment variables are required for Supabase operations.\")\n",
        "\n",
        "# ========== Initialize Supabase client ==========\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# ========== Helper functions ==========\n",
        "def safe_get(d, *keys, default=None):\n",
        "    for k in keys:\n",
        "        if not d: break\n",
        "        d = d.get(k, None)\n",
        "    return d if d is not None else default\n",
        "\n",
        "def normalize_title(t):\n",
        "    if not t: return \"\"\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    t = re.sub(r'[^a-z0-9 ]', '', t)\n",
        "    return t.strip()\n",
        "\n",
        "# ========== Semantic Scholar search ==========\n",
        "SEM_BASE = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "def semantic_search(query, limit=20):\n",
        "    if not SEM_KEY:\n",
        "        return []\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"limit\": limit,\n",
        "        \"offset\": 0,\n",
        "        \"fields\": \"paperId,title,authors,year,abstract,url,externalIds,openAccessPdf,citationCount\"\n",
        "    }\n",
        "    # Try Authorization header first, fallback to x-api-key\n",
        "    headers = {\"Authorization\": f\"Bearer {SEM_KEY}\"}\n",
        "    r = requests.get(SEM_BASE, params=params, headers=headers, timeout=30)\n",
        "    if r.status_code == 401:\n",
        "        headers = {\"x-api-key\": SEM_KEY}\n",
        "        r = requests.get(SEM_BASE, params=params, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    resp = r.json()\n",
        "    items = resp.get(\"data\", [])\n",
        "    results = []\n",
        "    for p in items:\n",
        "        pdf_candidates = []\n",
        "        oa = p.get(\"openAccessPdf\") or {}\n",
        "        if isinstance(oa, dict) and oa.get(\"url\"):\n",
        "            pdf_candidates.append(oa.get(\"url\"))\n",
        "        # Traditional canonical link\n",
        "        if p.get(\"url\"):\n",
        "            pdf_candidates.append(p.get(\"url\"))\n",
        "        results.append({\n",
        "            \"title\": p.get(\"title\"),\n",
        "            \"authors\": [a.get(\"name\") for a in p.get(\"authors\", []) if a.get(\"name\")],\n",
        "            \"year\": p.get(\"year\"),\n",
        "            \"doi\": safe_get(p, \"externalIds\", \"DOI\"),\n",
        "            \"externalIds\": p.get(\"externalIds\"),\n",
        "            \"abstract\": p.get(\"abstract\"),\n",
        "            \"citationCount\": p.get(\"citationCount\"),\n",
        "            \"pdf_candidates\": pdf_candidates,\n",
        "            \"source\": \"SemanticScholar\",\n",
        "            \"raw\": p\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# ========== SerpApi (Google Scholar) search (optional) ==========\n",
        "def serpapi_scholar_search(query, num=10):\n",
        "    if not SERPAPI_KEY:\n",
        "        return []\n",
        "    endpoint = \"https://serpapi.com/search\"\n",
        "    params = {\"engine\": \"google_scholar\", \"q\": query, \"api_key\": SERPAPI_KEY, \"num\": num}\n",
        "    r = requests.get(endpoint, params=params, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "    out = []\n",
        "    for item in j.get(\"organic_results\", []):\n",
        "        title = item.get(\"title\")\n",
        "        link = item.get(\"link\") or item.get(\"resources_link\") or \"\"\n",
        "        pdf_candidates = []\n",
        "        # If link directly ends with pdf or contains arxiv/pdf, add it\n",
        "        if link and (link.lower().endswith(\".pdf\") or \"arxiv.org/pdf\" in link.lower()):\n",
        "            pdf_candidates.append(link)\n",
        "        out.append({\n",
        "            \"title\": title,\n",
        "            \"authors\": None,\n",
        "            \"year\": None,\n",
        "            \"doi\": None,\n",
        "            \"externalIds\": {},\n",
        "            \"abstract\": item.get(\"snippet\"),\n",
        "            \"citationCount\": None,\n",
        "            \"pdf_candidates\": pdf_candidates,\n",
        "            \"source\": \"SerpApi\",\n",
        "            \"raw\": item\n",
        "        })\n",
        "    return out\n",
        "\n",
        "# ========== Merge & deduplicate logic ==========\n",
        "def merge_records(lists, fuzz_threshold=88):\n",
        "    \"\"\"\n",
        "    lists: list of lists (each source)\n",
        "    returns: merged list of dicts with merged metadata and pdf_candidates\n",
        "    \"\"\"\n",
        "    merged = []\n",
        "    for record in (r for src in lists for r in src):\n",
        "        # canonical key: DOI (if exists) else normalized title\n",
        "        doi = (record.get(\"doi\") or \"\").strip() or None\n",
        "        title_norm = normalize_title(record.get(\"title\") or \"\")\n",
        "        matched = None\n",
        "\n",
        "        # First try exact DOI match\n",
        "        if doi:\n",
        "            for m in merged:\n",
        "                if m.get(\"doi\") and m.get(\"doi\").lower() == doi.lower():\n",
        "                    matched = m\n",
        "                    break\n",
        "\n",
        "        # Then try exact normalized title\n",
        "        if not matched and title_norm:\n",
        "            for m in merged:\n",
        "                if normalize_title(m.get(\"title\") or \"\") == title_norm:\n",
        "                    matched = m\n",
        "                    break\n",
        "\n",
        "        # Finally fuzzy title match\n",
        "        if not matched and title_norm:\n",
        "            for m in merged:\n",
        "                score = fuzz.token_set_ratio(title_norm, normalize_title(m.get(\"title\") or \"\"))\n",
        "                if score >= fuzz_threshold:\n",
        "                    matched = m\n",
        "                    break\n",
        "\n",
        "        if matched:\n",
        "            # merge fields\n",
        "            matched[\"sources\"] = list(set(matched.get(\"sources\", []) + [record.get(\"source\")]))\n",
        "            # merge pdf candidates (unique)\n",
        "            for pdf in record.get(\"pdf_candidates\", []):\n",
        "                if pdf and pdf not in matched[\"pdf_candidates\"]:\n",
        "                    matched[\"pdf_candidates\"].append(pdf)\n",
        "            # prefer longer abstract\n",
        "            if (record.get(\"abstract\") or \"\") and len(record.get(\"abstract\") or \"\") > len(matched.get(\"abstract\") or \"\"):\n",
        "                matched[\"abstract\"] = record.get(\"abstract\")\n",
        "            # keep max citation count\n",
        "            if record.get(\"citationCount\") and (not matched.get(\"citationCount\") or record.get(\"citationCount\") > matched.get(\"citationCount\")):\n",
        "                matched[\"citationCount\"] = record.get(\"citationCount\")\n",
        "            # fill missing doi/authors/year\n",
        "            for k in (\"doi\",\"authors\",\"year\",\"externalIds\"):\n",
        "                if not matched.get(k) and record.get(k):\n",
        "                    matched[k] = record.get(k)\n",
        "        else:\n",
        "            # new entry\n",
        "            rec = {\n",
        "                \"title\": record.get(\"title\"),\n",
        "                \"authors\": record.get(\"authors\") or [],\n",
        "                \"year\": record.get(\"year\"),\n",
        "                \"doi\": record.get(\"doi\"),\n",
        "                \"externalIds\": record.get(\"externalIds\") or {},\n",
        "                \"abstract\": record.get(\"abstract\"),\n",
        "                \"citationCount\": record.get(\"citationCount\"),\n",
        "                \"pdf_candidates\": [p for p in record.get(\"pdf_candidates\", []) if p],\n",
        "                \"sources\": [record.get(\"source\")] if record.get(\"source\") else [],\n",
        "                \"raws\": [record.get(\"raw\")]\n",
        "            }\n",
        "            merged.append(rec)\n",
        "    return merged\n",
        "\n",
        "# ========== PDF utilities ==========\n",
        "def download_pdf_bytes(url, timeout=60):\n",
        "    # simple HEAD check to ensure pdf likely downloadable\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout, stream=True)\n",
        "        r.raise_for_status()\n",
        "        content_type = r.headers.get(\"content-type\",\"\")\n",
        "        # Accept pdf or octet-stream responses; otherwise still attempt if link ends with .pdf\n",
        "        if (\"pdf\" in content_type) or url.lower().endswith(\".pdf\") or \"arxiv.org/pdf\" in url.lower():\n",
        "            return r.content\n",
        "        else:\n",
        "            # still return bytes but caller should decide\n",
        "            return r.content\n",
        "    except Exception as e:\n",
        "        raise\n",
        "\n",
        "def extract_text_from_pdf_bytes(pdf_bytes, max_pages=None):\n",
        "    try:\n",
        "        text_pages = []\n",
        "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
        "            pages = pdf.pages if max_pages is None else pdf.pages[:max_pages]\n",
        "            for p in pages:\n",
        "                t = p.extract_text() or \"\"\n",
        "                text_pages.append(t)\n",
        "        return \"\\n\".join(text_pages)\n",
        "    except Exception as e:\n",
        "        print(\"pdfplumber error:\", e)\n",
        "        return \"\"\n",
        "\n",
        "# ========== Supabase storage helpers ==========\n",
        "def ensure_bucket(bucket_name):\n",
        "    # Try listing buckets and create if not present (may require service_role)\n",
        "    try:\n",
        "        buckets = supabase.storage.list_buckets()\n",
        "        # supabase-py returns a dict with 'data' or raw list depending on version\n",
        "        existing = []\n",
        "        if isinstance(buckets, dict):\n",
        "            existing = [b[\"name\"] for b in buckets.get(\"data\", [])]\n",
        "        elif isinstance(buckets, list):\n",
        "            existing = [b.get(\"name\") for b in buckets]\n",
        "        if bucket_name not in existing:\n",
        "            print(f\"Bucket '{bucket_name}' not found. Attempting to create it (requires service_role key).\")\n",
        "            try:\n",
        "                res = supabase.storage.create_bucket(bucket_name, public=True)\n",
        "                print(\"Bucket create response:\", res)\n",
        "            except Exception as e:\n",
        "                print(\"Could not create bucket programmatically - create it in the Supabase dashboard manually.\", e)\n",
        "        else:\n",
        "            print(f\"Bucket '{bucket_name}' exists.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error checking/creating bucket:\", e)\n",
        "\n",
        "def upload_pdf_to_supabase(bucket, path, pdf_bytes, content_type=\"application/pdf\"):\n",
        "    # supabase.storage.from_(bucket).upload expects a file-like or bytes depending on client version\n",
        "    try:\n",
        "        res = supabase.storage.from_(bucket).upload(path, pdf_bytes, {\"content-type\": content_type})\n",
        "        return res\n",
        "    except Exception as e:\n",
        "        raise\n",
        "\n",
        "# ========== Insert metadata into Supabase table ==========\n",
        "def insert_paper_row(table, metadata):\n",
        "    try:\n",
        "        r = supabase.table(table).insert(metadata).execute()\n",
        "        return r\n",
        "    except Exception as e:\n",
        "        # If RLS error occurs, supabase client often returns dict-like error\n",
        "        print(\"Insert error:\", e)\n",
        "        raise\n",
        "\n",
        "# ========== Main pipeline: search -> merge -> download -> upload -> insert ==========\n",
        "def run_pipeline(topic, limit_sem=10, limit_serp=10, bucket=\"academic_pdfs\", table=\"papers\", max_pdf_pages=3, pause_between_downloads=1.0):\n",
        "    # 1) Discover\n",
        "    print(\"Searching Semantic Scholar...\")\n",
        "    sem = semantic_search(topic, limit=limit_sem)\n",
        "    print(f\"Semantic Scholar returned {len(sem)} items.\")\n",
        "    serp = []\n",
        "    if SERPAPI_KEY:\n",
        "        try:\n",
        "            print(\"Searching SerpApi (Google Scholar)...\")\n",
        "            serp = serpapi_scholar_search(topic, num=limit_serp)\n",
        "            print(f\"SerpApi returned {len(serp)} items.\")\n",
        "        except Exception as e:\n",
        "            print(\"SerpApi search failed:\", e)\n",
        "\n",
        "    # 2) Merge\n",
        "    merged = merge_records([sem, serp])\n",
        "    print(f\"Merged to {len(merged)} unique records (approx).\")\n",
        "\n",
        "    # ensure bucket exists\n",
        "    ensure_bucket(bucket)\n",
        "\n",
        "    # 3) Process each record\n",
        "    logs = []\n",
        "    for i, rec in enumerate(merged):\n",
        "        print(f\"\\n[{i+1}/{len(merged)}] {rec.get('title')[:120]}\")\n",
        "        status = {\"title\": rec.get(\"title\"), \"doi\": rec.get(\"doi\"), \"uploaded\": False, \"inserted\": False, \"message\": None}\n",
        "        pdf_ok = False\n",
        "        for pdf_url in rec.get(\"pdf_candidates\", []):\n",
        "            if not pdf_url:\n",
        "                continue\n",
        "            print(\"Attempting PDF URL:\", pdf_url)\n",
        "            try:\n",
        "                pdf_bytes = download_pdf_bytes(pdf_url)\n",
        "                # quick sanity: check size\n",
        "                if not pdf_bytes or len(pdf_bytes) < 2000:\n",
        "                    print(\"Downloaded content small or missing â€” skipping this PDF candidate.\")\n",
        "                    continue\n",
        "                # make filename\n",
        "                safe_name = rec.get(\"doi\") or rec.get(\"title\") or f\"paper_{i}\"\n",
        "                safe_name = re.sub(r'[^\\w\\-_. ]', '_', safe_name)[:120]\n",
        "                fname = f\"{safe_name}.pdf\"\n",
        "                # upload\n",
        "                print(\"Uploading to Supabase storage:\", fname)\n",
        "                upload_res = upload_pdf_to_supabase(bucket, fname, pdf_bytes)\n",
        "                print(\"Upload response:\", upload_res)\n",
        "                status[\"uploaded_path\"] = fname\n",
        "                status[\"uploaded\"] = True\n",
        "                # extract text (first max_pdf_pages)\n",
        "                excerpt = extract_text_from_pdf_bytes(pdf_bytes, max_pages=max_pdf_pages)\n",
        "                status[\"text_excerpt\"] = excerpt[:2000] if excerpt else \"\"\n",
        "                # prepare metadata\n",
        "                metadata = {\n",
        "                    \"title\": rec.get(\"title\"),\n",
        "                    \"authors\": rec.get(\"authors\"),\n",
        "                    \"year\": rec.get(\"year\"),\n",
        "                    \"doi\": rec.get(\"doi\"),\n",
        "                    \"source\": \",\".join(rec.get(\"sources\", [])),\n",
        "                    \"external_ids\": rec.get(\"externalIds\"),\n",
        "                    \"supabase_path\": fname,\n",
        "                    \"pdf_downloaded\": True,\n",
        "                    \"text_excerpt\": status.get(\"text_excerpt\")\n",
        "                }\n",
        "                # insert row\n",
        "                print(\"Inserting metadata row into table:\", table)\n",
        "                ins_res = insert_paper_row(table, metadata)\n",
        "                print(\"Insert response:\", ins_res)\n",
        "                status[\"inserted\"] = True\n",
        "                status[\"insert_response\"] = ins_res\n",
        "                pdf_ok = True\n",
        "                time.sleep(pause_between_downloads)\n",
        "                break  # stop on first successful pdf\n",
        "            except requests.HTTPError as e:\n",
        "                print(\"HTTP error downloading PDF:\", e)\n",
        "            except Exception as e:\n",
        "                # bubble up RLS error messages prefixed from supabase API\n",
        "                print(\"Error processing PDF/Upload/Insert:\", e)\n",
        "                status[\"message\"] = str(e)\n",
        "                # If this is an RLS error, surface it cleanly:\n",
        "                if \"row-level security\" in str(e).lower() or \"violates row-level\" in str(e).lower():\n",
        "                    print(\"ðŸš¨ Row-Level Security (RLS) prevented insertion. Use service_role key or create a policy allowing inserts.\")\n",
        "                    # Do not re-raise here; record log and continue\n",
        "                # continue to next candidate\n",
        "        if not pdf_ok:\n",
        "            print(\"No valid PDF processed for this record.\")\n",
        "            status[\"message\"] = status.get(\"message\") or \"No valid PDF found or all downloads failed.\"\n",
        "        logs.append(status)\n",
        "\n",
        "    # Save logs to a JSON file in Colab workspace\n",
        "    with open(\"pipeline_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "    print(\"\\nPipeline finished. Log saved to pipeline_log.json\")\n",
        "    return logs\n",
        "\n",
        "# ========== Usage example ==========\n",
        "# Replace the topic and limits as needed. If you don't have SERPAPI_KEY leave it blank.\n",
        "topic = \"cloud computing\"  # change to your research topic\n",
        "logs = run_pipeline(topic, limit_sem=5, limit_serp=7, bucket=\"academic_pdfs\", table=\"papers\", max_pdf_pages=2)\n",
        "\n",
        "# Display a short summary of logs\n",
        "successful = [l for l in logs if l.get(\"inserted\")]\n",
        "failed = [l for l in logs if not l.get(\"inserted\")]\n",
        "print(f\"\\nInserted rows: {len(successful)}, Failed inserts: {len(failed)}\")\n",
        "pprint(successful[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_XG7i9Shmej8",
        "outputId": "612ae030-deeb-4e41-c774-b6c289007cee"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.22.0)\n",
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.12/dist-packages (1.7.11)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: realtime in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-functions in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: storage3 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-auth in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: postgrest in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.13.5)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.4.3)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.2.18)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.12/dist-packages (from scholarly) (2.2.0)\n",
            "Requirement already satisfied: free-proxy in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.1.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.1.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.36.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.12/dist-packages (from scholarly) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.15.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.14.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from arrow->scholarly) (2.9.0.post0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow->scholarly) (2.9.0.20251008)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->scholarly) (2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from bibtexparser->scholarly) (3.2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->scholarly) (1.17.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from free-proxy->scholarly) (5.4.0)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.11.10)\n",
            "Requirement already satisfied: yarl>=1.20.1 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (1.22.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime->supabase) (15.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.31.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (1.9.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.10.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions->supabase) (0.4.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.1.0->postgrest->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scholarly) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket<1.0,>=0.12.2->selenium->scholarly) (1.2.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (0.3.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.3)\n",
            "Searching Semantic Scholar...\n",
            "Semantic Scholar returned 5 items.\n",
            "Searching SerpApi (Google Scholar)...\n",
            "SerpApi returned 7 items.\n",
            "Merged to 6 unique records (approx).\n",
            "Error checking/creating bucket: 'SyncBucket' object has no attribute 'get'\n",
            "\n",
            "[1/6] The NIST Definition of Cloud Computing\n",
            "Attempting PDF URL: https://doi.org/10.6028/nist.sp.800-145\n",
            "Uploading to Supabase storage: 10.6028_NIST.SP.800-145.pdf\n",
            "Upload response: UploadResponse(path=('10.6028_NIST.SP.800-145.pdf',), full_path='academic_pdfs/10.6028_NIST.SP.800-145.pdf', fullPath='academic_pdfs/10.6028_NIST.SP.800-145.pdf')\n",
            "Inserting metadata row into table: papers\n",
            "Insert response: data=[{'id': '1d07c19c-f32d-4eed-8103-eb83b0edcf72', 'title': 'The NIST Definition of Cloud Computing', 'authors': ['P. Mell', 'T. Grance'], 'year': 2011, 'doi': '10.6028/NIST.SP.800-145', 'source': 'SerpApi,SemanticScholar', 'external_ids': {'DOI': '10.6028/NIST.SP.800-145', 'MAG': '2275530856', 'CorpusId': 167781982}, 'supabase_path': '10.6028_NIST.SP.800-145.pdf', 'pdf_downloaded': True, 'text_excerpt': 'Special Publication 800-145\\nThe NIST Definition of Cloud\\nComputing\\nRecommendations of the National Institute\\nof Standards and Technology\\nPeter Mell\\nTimothy Grance\\nNIST Special Publication 800-145 The NIST Definition of Cloud Computing\\nPeter Mell\\nTimothy Grance\\nC O M P U T E R S E C U R I T Y\\nComputer Security Division\\nInformation Technology Laboratory\\nNational Institute of Standards and Technology\\nGaithersburg, MD 20899-8930\\nSeptember 2011\\nU.S. Department of Commerce\\nRebecca M. Blank, Acting Secretary\\nNational Institute of Standards and Technology\\nPatrick D. Gallagher, Under Secretary for Standards and\\nTechnology and Director', 'created_at': '2025-10-12T09:05:50.792259+00:00'}] count=None\n",
            "\n",
            "[2/6] A view of cloud computing\n",
            "Attempting PDF URL: https://dl.acm.org/doi/pdf/10.1145/1721654.1721672\n",
            "HTTP error downloading PDF: 403 Client Error: Forbidden for url: https://dl.acm.org/doi/pdf/10.1145/1721654.1721672\n",
            "Attempting PDF URL: https://www.semanticscholar.org/paper/5deef74e922df23a636a3fd4e33c119247de8d30\n",
            "Downloaded content small or missing â€” skipping this PDF candidate.\n",
            "Attempting PDF URL: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/spe.995\n",
            "HTTP error downloading PDF: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/spe.995\n",
            "Attempting PDF URL: https://www.semanticscholar.org/paper/30a82a63a339c1e69aac36b23900544fe9ec97bb\n",
            "Downloaded content small or missing â€” skipping this PDF candidate.\n",
            "Attempting PDF URL: https://www.semanticscholar.org/paper/1f7190fc294246f83f1f331cc51e3264851d0d36\n",
            "Downloaded content small or missing â€” skipping this PDF candidate.\n",
            "No valid PDF processed for this record.\n",
            "\n",
            "[3/6] Improved wild horse optimization with levy flight algorithm for effective task scheduling in cloud computing\n",
            "Attempting PDF URL: https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-023-00401-1\n",
            "Uploading to Supabase storage: 10.1186_s13677-023-00401-1.pdf\n",
            "Upload response: UploadResponse(path=('10.1186_s13677-023-00401-1.pdf',), full_path='academic_pdfs/10.1186_s13677-023-00401-1.pdf', fullPath='academic_pdfs/10.1186_s13677-023-00401-1.pdf')\n",
            "Inserting metadata row into table: papers\n",
            "Insert response: data=[{'id': 'a1e04bee-7688-4bfb-8e0b-9b2f34a866aa', 'title': 'Improved wild horse optimization with levy flight algorithm for effective task scheduling in cloud computing', 'authors': ['G. Saravanan', 'Neelakandan Subramani', 'P. Ezhumalai', 'Dr Sudhanshu Maurya'], 'year': 2023, 'doi': '10.1186/s13677-023-00401-1', 'source': 'SemanticScholar', 'external_ids': {'DOI': '10.1186/s13677-023-00401-1', 'DBLP': 'journals/jcloudc/SaravananNEM23', 'CorpusId': 257042830}, 'supabase_path': '10.1186_s13677-023-00401-1.pdf', 'pdf_downloaded': True, 'text_excerpt': 'Saravanan et al. Journal of Cloud Computing (2023) 12:24 Journal of Cloud Computing:\\nhttps://doi.org/10.1186/s13677-023-00401-1\\nAdvances, Systems and Applications\\nRESEARCH Open Access\\nImproved wild horse optimization with levy\\nflight algorithm for effective task scheduling\\nin cloud computing\\nG. Saravanan1, S. Neelakandan2*, P. Ezhumalai3 and Sudhanshu Maurya4\\nAbstract\\nCloud Computing, the efficiency of task scheduling is proportional to the effectiveness of users. The improved sched-\\nuling efficiency algorithm (also known as the improved Wild Horse Optimization, or IWHO) is proposed to address\\nthe problems of lengthy scheduling time, high-cost consumption, and high virtual machine load in cloud computing\\ntask scheduling. First, a cloud computing task scheduling and distribution model is built, with time, cost, and virtual\\nmachines as the primary factors. Second, a feasible plan for each whale individual corresponding to cloud computing\\ntask scheduling is to find the best whale individual, which is the best feasible plan; to better find the optimal indi-\\nvidual, we use the inertial weight strategy for the Improved whale optimization algorithm to improve the local search\\nability and effectively prevent the algorithm from reaching premature convergence. To deliver services and access to\\nshared resources, Cloud Computing (CC) employs a cloud service provider (CSP). In a CC context, task scheduling has\\na significant impact on resource utilization and overall system performance. It is a Nondeterministic Polynomial (NP)-\\nhard problem that is solved using metaheuristic optimization techniques to improve the effectiveness of job schedul-\\ning in a CC environment. This incentive is used in this study to provide the Improved Wild Horse Optimization with\\nLevy Flight Algorithm for Task Scheduling in cloud computing (IWHOLF-TSC) approach, which is an improved wild\\nhorse optimization with levy flight algorithm for cloud task scheduling. Task scheduling can be addressed in the cloud', 'created_at': '2025-10-12T09:06:17.484+00:00'}] count=None\n",
            "\n",
            "[4/6] Cloud computing: Today and tomorrow.\n",
            "Attempting PDF URL: https://www.jot.fm/issues/issue_2009_01/column4.pdf\n",
            "Uploading to Supabase storage: Cloud computing_ Today and tomorrow..pdf\n",
            "Upload response: UploadResponse(path=('Cloud computing_ Today and tomorrow..pdf',), full_path='academic_pdfs/Cloud computing_ Today and tomorrow..pdf', fullPath='academic_pdfs/Cloud computing_ Today and tomorrow..pdf')\n",
            "Inserting metadata row into table: papers\n",
            "Insert response: data=[{'id': '8fe77089-9165-48f8-b922-8fc1b30f9698', 'title': 'Cloud computing: Today and tomorrow.', 'authors': [], 'year': None, 'doi': None, 'source': 'SerpApi', 'external_ids': {}, 'supabase_path': 'Cloud computing_ Today and tomorrow..pdf', 'pdf_downloaded': True, 'text_excerpt': 'J O T\\nOURNAL OF BJECT ECHNOLOGY\\nOnline at http://www.jot.fm. Published by ETH Zurich, Chair of Software Engineering Â©JOT, 2009\\nVol. 8, No. 1, January-February 2009\\nCloud Computing: Today and Tomorrow\\nWon Kim, Sungkyunkwan University, Suwon, S. Korea\\nAbstract\\nDuring the past few years, cloud computing has become a key IT buzzword. Although\\nthe definition of cloud computing is still â€œcloudyâ€, the trade press and bloggers label\\nmany vendors as cloud computing vendors, and report on their services and issues.\\nCloud computing is in its infancy in terms of market adoption. However, it is a key IT\\nmegatrend that will take root. This article reviews its definition and status, adoption\\nissues, and provides a glimpse of its future and discusses technical issues that are\\nexpected to be addressed.\\n1 STATUS\\nPerhaps the simplest working definition of cloud computing is â€œbeing able to access files,\\ndata, programs and 3rd party services from a Web browser via the Internet that are hosted\\nby a 3rd party providerâ€[Hodson08] and â€œpaying only for the computing resources and\\nservices usedâ€.\\nOften cloud computing is used synonymously, inaccurately in my view, with such\\nterms as utility computing (or on-demand computing), software as a service (SaaS), and\\ngrid computing. Of these, as I will show below, utility computing and SaaS are merely\\ntwo of several forms of service cloud computing can provide. Grid computing is simply\\none type of underlying technologies for implementing cloud computing.\\nThe term â€œcloudâ€ in cloud computing is used synomously with â€œdata centerâ€. Today\\nthe computing field is able to envision transitioning into the cloud computing era because\\nof the breath-taking advances in computing and information technologies during the past\\nthree decades. The advances include the buildup of the Internet backbone, the widespread\\nadoption of broadband access to the Internet, the powerful network of servers and storage\\nin data centers, the advances in high performance and scalable sof', 'created_at': '2025-10-12T09:06:20.924589+00:00'}] count=None\n",
            "\n",
            "[5/6] The characteristics of cloud computing\n",
            "No valid PDF processed for this record.\n",
            "\n",
            "[6/6] Cloud computing: opportunities and challenges\n",
            "No valid PDF processed for this record.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type APIResponse is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4184054651.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# Replace the topic and limits as needed. If you don't have SERPAPI_KEY leave it blank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cloud computing\"\u001b[0m  \u001b[0;31m# change to your research topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_sem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_serp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"academic_pdfs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"papers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_pdf_pages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;31m# Display a short summary of logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4184054651.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(topic, limit_sem, limit_serp, bucket, table, max_pdf_pages, pause_between_downloads)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# Save logs to a JSON file in Colab workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pipeline_log.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPipeline finished. Log saved to pipeline_log.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0m_floatstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type APIResponse is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from supabase import create_client\n",
        "\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://bjquctxzjruejzbdxjfr.supabase.co\"\n",
        "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImJqcXVjdHh6anJ1ZWp6YmR4amZyIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MDA5MTIyMiwiZXhwIjoyMDc1NjY3MjIyfQ.llYmlbVzezEpcAMA5agzPkovdzFlVv0Kxy8s_ikXvmI\"\n",
        "\n",
        "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]\n",
        "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# quick verify\n",
        "print(\"Buckets:\", supabase.storage.list_buckets())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKtiCJFWplJB",
        "outputId": "29125edf-f9d0-4111-ae09-1571ee390616"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buckets: [SyncBucket(id='academic_pdfs', name='academic_pdfs', owner='', public=True, created_at=datetime.datetime(2025, 10, 12, 8, 34, 33, 894000, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2025, 10, 12, 8, 34, 33, 894000, tzinfo=TzInfo(UTC)), file_size_limit=None, allowed_mime_types=None, type='STANDARD')]\n"
          ]
        }
      ]
    }
  ]
}